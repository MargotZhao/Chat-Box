{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb8c6fa7",
   "metadata": {},
   "source": [
    "# microsoft/Phi-3-mini-4k-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765ef8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    "    prepare_model_for_kbit_training\n",
    ")\n",
    "\n",
    "# Step 1: Load the dataset (same as before)\n",
    "df = pd.read_csv(r\"C:\\\\Graduate courses\\\\s2\\\\Gen AI\\\\therpist\\\\train.csv\")\n",
    "print(f\"Dataset loaded with {len(df)} examples\")\n",
    "print(f\"Original model parameters that would be trained: millions\")\n",
    "\n",
    "# Process dataset (same as before)\n",
    "def process_conversations(example):\n",
    "    return {\"formatted_text\": example[\"conversations\"]}\n",
    "\n",
    "dataset = Dataset.from_pandas(df)\n",
    "processed_dataset = dataset.map(process_conversations)\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\", trust_remote_code=True)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"formatted_text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "tokenized_dataset = processed_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"conversations\", \"id\", \"formatted_text\"])\n",
    "tokenized_dataset = tokenized_dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "# Load model in 8-bit precision to save memory\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\", \n",
    "    trust_remote_code=True,\n",
    "    load_in_8bit=True,\n",
    ")\n",
    "\n",
    "# Count original trainable parameters\n",
    "original_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Original trainable parameters: {original_params:,}\")\n",
    "\n",
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=8,                     # Rank of the update matrices\n",
    "    lora_alpha=32,           # Parameter for scaling\n",
    "    lora_dropout=0.1,        # Dropout probability for LoRA layers\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"], # Target attention modules\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "# Prepare model for LoRA fine-tuning\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Count LoRA trainable parameters\n",
    "lora_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"LoRA trainable parameters: {lora_params:,}\")\n",
    "print(f\"Parameter reduction: {100 * (1 - lora_params / original_params):.2f}%\")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results/phi3-lora-mental-health\",\n",
    "    per_device_train_batch_size=8,   # Can use larger batches with LoRA\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=100,\n",
    "    max_steps=1000,\n",
    "    learning_rate=3e-4,\n",
    "    fp16=True,\n",
    "    logging_steps=20,\n",
    "    save_steps=200,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# Setup trainer\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, \n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the LoRA adapter only (much smaller than full model)\n",
    "model.save_pretrained(\"./lora-phi3-mental-health-adapter\")\n",
    "print(\"LoRA adapter saved successfully\")\n",
    "\n",
    "# Display parameter reduction statistics\n",
    "print(f\"Full model parameters: {original_params:,}\")\n",
    "print(f\"LoRA-only parameters: {lora_params:,}\")\n",
    "print(f\"Storage reduction: {(original_params - lora_params) / 1_000_000:.2f}M parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0d4224",
   "metadata": {},
   "source": [
    "# google/gemma-2-2b-it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792145c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    "    prepare_model_for_kbit_training\n",
    ")\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "print(\"Loading dataset...\")\n",
    "df = pd.read_csv(r\"C:\\Graduate courses\\s2\\Gen AI\\therpist\\train.csv\")\n",
    "print(f\"Dataset loaded with {len(df)} examples\")\n",
    "\n",
    "# Step 2: Process dataset\n",
    "def process_conversations(example):\n",
    "    # Process each conversation into the format Gemma expects\n",
    "    # Gemma uses a specific format for chat: <start_of_turn>user\\nmessage<end_of_turn>\n",
    "    conversation = example[\"conversations\"]\n",
    "    # You may need to adjust this formatting based on your data structure\n",
    "    return {\"formatted_text\": conversation}\n",
    "\n",
    "dataset = Dataset.from_pandas(df)\n",
    "processed_dataset = dataset.map(process_conversations)\n",
    "\n",
    "# Step 3: Tokenize dataset\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"formatted_text\"], \n",
    "        truncation=True, \n",
    "        padding=\"max_length\", \n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "print(\"Tokenizing dataset...\")\n",
    "tokenized_dataset = processed_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"conversations\", \"id\", \"formatted_text\"])\n",
    "tokenized_dataset = tokenized_dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "# Step 4: Load model in 8-bit to save memory\n",
    "print(\"Loading base model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-2-2b-it\",\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Calculate original trainable parameters\n",
    "original_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_original = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters in model: {original_params:,}\")\n",
    "print(f\"Trainable parameters before LoRA: {trainable_original:,}\")\n",
    "\n",
    "# Step 5: Configure LoRA\n",
    "print(\"Applying LoRA configuration...\")\n",
    "# For Gemma, target the appropriate modules - this may need adjustment\n",
    "target_modules = [\n",
    "    \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "    \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "]\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=16,                   # Rank of the update matrices\n",
    "    lora_alpha=32,          # Parameter for scaling\n",
    "    lora_dropout=0.05,      # Dropout probability\n",
    "    target_modules=target_modules,\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "# Prepare model for LoRA fine-tuning\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()  # This will print parameter info directly\n",
    "\n",
    "# Calculate LoRA trainable parameters manually\n",
    "lora_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Trainable parameters with LoRA: {lora_params:,}\")\n",
    "print(f\"Parameter reduction: {100 * (1 - lora_params / original_params):.4f}%\")\n",
    "print(f\"Memory savings: ~{(original_params - lora_params) / 1_000_000:.2f}M parameters\")\n",
    "\n",
    "# Step 6: Training arguments\n",
    "print(\"Setting up training...\")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results/gemma-lora-mental-health\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=100,\n",
    "    max_steps=1000,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    logging_steps=20,\n",
    "    save_steps=200,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# Step 7: Setup trainer\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, \n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Step 8: Train the model\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "# Step 9: Save the LoRA adapter only\n",
    "adapter_path = \"./gemma-lora-mental-health-adapter\"\n",
    "model.save_pretrained(adapter_path)\n",
    "tokenizer.save_pretrained(adapter_path)\n",
    "print(f\"LoRA adapter saved to {adapter_path}\")\n",
    "\n",
    "# Step 10: Display parameter statistics for comparison\n",
    "print(\"\\n==== PARAMETER EFFICIENCY SUMMARY ====\")\n",
    "print(f\"Total model parameters: {original_params:,}\")\n",
    "print(f\"Parameters if fully fine-tuned: {trainable_original:,}\")\n",
    "print(f\"Parameters with LoRA: {lora_params:,}\")\n",
    "print(f\"Parameter reduction: {100 * (1 - lora_params / original_params):.4f}%\")\n",
    "print(f\"Storage size reduction: ~{(original_params - lora_params) * 4 / (1024*1024):.2f} MB\")\n",
    "print(\"=====================================\")\n",
    "\n",
    "# Test the model with a sample input\n",
    "test_input = \"I've been feeling really anxious lately and can't sleep.\"\n",
    "messages = [{\"role\": \"user\", \"content\": test_input}]\n",
    "\n",
    "# Load the fine-tuned model with adapter for inference\n",
    "from peft import PeftModel, PeftConfig\n",
    "config = PeftConfig.from_pretrained(adapter_path)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "fine_tuned_model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "\n",
    "from transformers import pipeline\n",
    "pipe = pipeline(\"text-generation\", model=fine_tuned_model, tokenizer=tokenizer)\n",
    "response = pipe(messages)\n",
    "print(\"\\nSample response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a8513c",
   "metadata": {},
   "source": [
    "# TinyLlama/TinyLlama-1.1B-Chat-v1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7442b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    "    prepare_model_for_kbit_training\n",
    ")\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "print(\"Loading dataset...\")\n",
    "df = pd.read_csv(r\"C:\\Graduate courses\\s2\\Gen AI\\therpist\\train.csv\")\n",
    "print(f\"Dataset loaded with {len(df)} examples\")\n",
    "\n",
    "# Step 2: Process dataset\n",
    "def process_conversations(example):\n",
    "    # TinyLlama uses Llama 2 chat template\n",
    "    # Format: <|system|>\\n{system_message}<|user|>\\n{user_message}<|assistant|>\\n{assistant_message}\n",
    "    conversation = example[\"conversations\"]\n",
    "    # This is a placeholder - adjust based on your exact data format\n",
    "    return {\"formatted_text\": conversation}\n",
    "\n",
    "dataset = Dataset.from_pandas(df)\n",
    "processed_dataset = dataset.map(process_conversations)\n",
    "\n",
    "# Step 3: Tokenize dataset\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"formatted_text\"], \n",
    "        truncation=True, \n",
    "        padding=\"max_length\", \n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "print(\"Tokenizing dataset...\")\n",
    "tokenized_dataset = processed_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"conversations\", \"id\", \"formatted_text\"])\n",
    "tokenized_dataset = tokenized_dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "# Step 4: Load model\n",
    "print(\"Loading base model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    load_in_8bit=True,  # Use 8-bit quantization to save memory\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Calculate original parameters\n",
    "original_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_original = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters in model: {original_params:,}\")\n",
    "print(f\"Trainable parameters before LoRA: {trainable_original:,}\")\n",
    "\n",
    "# Step 5: Configure LoRA\n",
    "print(\"Applying LoRA configuration...\")\n",
    "# These target modules are typical for Llama architecture models\n",
    "target_modules = [\n",
    "    \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "    \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "]\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=8,                    # Rank - can be smaller for this smaller model\n",
    "    lora_alpha=16,          # Parameter for scaling\n",
    "    lora_dropout=0.05,      # Dropout probability\n",
    "    target_modules=target_modules,\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "# Prepare model for LoRA fine-tuning\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()  # Built-in function to show parameter stats\n",
    "\n",
    "# Calculate LoRA trainable parameters\n",
    "lora_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Trainable parameters with LoRA: {lora_params:,}\")\n",
    "print(f\"Parameter reduction: {100 * (1 - lora_params / original_params):.4f}%\")\n",
    "print(f\"Memory savings: ~{(original_params - lora_params) / 1_000_000:.2f}M parameters\")\n",
    "\n",
    "# Step 6: Training arguments\n",
    "print(\"Setting up training...\")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results/tinyllama-lora-mental-health\",\n",
    "    per_device_train_batch_size=8,    # Can use larger batches for this smaller model\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=100,\n",
    "    max_steps=1000,          # Adjust based on dataset size\n",
    "    learning_rate=3e-4,\n",
    "    fp16=True,\n",
    "    logging_steps=20,\n",
    "    save_steps=200,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# Step 7: Setup trainer\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, \n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Step 8: Train the model\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "# Step 9: Save the LoRA adapter only\n",
    "adapter_path = \"./tinyllama-lora-mental-health-adapter\"\n",
    "model.save_pretrained(adapter_path)\n",
    "tokenizer.save_pretrained(adapter_path)\n",
    "print(f\"LoRA adapter saved to {adapter_path}\")\n",
    "\n",
    "# Step 10: Display parameter statistics\n",
    "print(\"\\n==== PARAMETER EFFICIENCY SUMMARY ====\")\n",
    "print(f\"Total model parameters: {original_params:,}\")\n",
    "print(f\"Parameters if fully fine-tuned: {trainable_original:,}\")\n",
    "print(f\"Parameters with LoRA: {lora_params:,}\")\n",
    "print(f\"Parameter reduction: {100 * (1 - lora_params / original_params):.4f}%\")\n",
    "print(f\"Storage size reduction: ~{(original_params - lora_params) * 2 / (1024*1024):.2f} MB\")\n",
    "print(\"=====================================\")\n",
    "\n",
    "# Test the model\n",
    "test_input = \"I've been feeling overwhelmed lately with work and personal life.\"\n",
    "\n",
    "# For TinyLlama chat format\n",
    "formatted_prompt = f\"<|user|>\\n{test_input}<|assistant|>\\n\"\n",
    "\n",
    "# Load the fine-tuned model with adapter for inference\n",
    "from peft import PeftModel, PeftConfig\n",
    "config = PeftConfig.from_pretrained(adapter_path)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "fine_tuned_model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "\n",
    "# Generate response\n",
    "inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(fine_tuned_model.device)\n",
    "outputs = fine_tuned_model.generate(\n",
    "    inputs.input_ids,\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9\n",
    ")\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"\\nSample response:\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI-model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
